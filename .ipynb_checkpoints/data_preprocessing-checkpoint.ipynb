{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preps Wisconsin Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"wdbc.data\", header=None)\n",
    "\n",
    "# Add column names\n",
    "column_names = [\n",
    "    \"id\", \"diagnosis\",\n",
    "    # Mean features\n",
    "    \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \n",
    "    \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \n",
    "    \"concave_points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\",\n",
    "    # SE features\n",
    "    \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \n",
    "    \"smoothness_se\", \"compactness_se\", \"concavity_se\", \n",
    "    \"concave_points_se\", \"symmetry_se\", \"fractal_dimension_se\",\n",
    "    # Worst features\n",
    "    \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \n",
    "    \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\", \n",
    "    \"concave_points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "data.columns = column_names\n",
    "\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Inspection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nTotal missing values:\", data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(data.dtypes)\n",
    "print(\"\\nBasic statistics:\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Number of duplicate rows: {data.duplicated().sum()}\")\n",
    "\n",
    "# Check target variable distribution\n",
    "print(\"\\nDiagnosis distribution:\")\n",
    "print(data['diagnosis'].value_counts())\n",
    "print(\"\\nDiagnosis percentages:\")\n",
    "print(data['diagnosis'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=data, x='diagnosis', palette='Set2')\n",
    "plt.title('Distribution of Diagnosis (M=Malignant, B=Benign)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "class_counts = data['diagnosis'].value_counts()\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"Moderate class imbalance detected - consider using class weights or resampling\")\n",
    "else:\n",
    "    print(\"Classes are relatively balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID column (not useful for prediction)\n",
    "data_clean = data.drop('id', axis=1)\n",
    "\n",
    "# Encode diagnosis: M (Malignant) = 1, B (Benign) = 0\n",
    "label_encoder = LabelEncoder()\n",
    "data_clean['diagnosis_encoded'] = label_encoder.fit_transform(data_clean['diagnosis'])\n",
    "\n",
    "print(\"Encoding mapping:\")\n",
    "for label, encoded in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):\n",
    "    print(f\"  {label} -> {encoded}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = data_clean.drop(['diagnosis', 'diagnosis_encoded'], axis=1)\n",
    "y = data_clean['diagnosis_encoded']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature names: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for highly correlated features (multicollinearity)\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Find pairs of highly correlated features (> 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr_pairs.append(\n",
    "                (correlation_matrix.columns[i], \n",
    "                 correlation_matrix.columns[j], \n",
    "                 correlation_matrix.iloc[i, j])\n",
    "            )\n",
    "\n",
    "print(f\"Number of highly correlated feature pairs (|r| > 0.9): {len(high_corr_pairs)}\")\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nTop 10 highly correlated pairs:\")\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation with target\n",
    "correlations_with_target = X.corrwith(y).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "correlations_with_target.plot(kind='barh', color='steelblue')\n",
    "plt.title('Feature Correlation with Diagnosis', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 features most correlated with malignancy:\")\n",
    "print(correlations_with_target.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain class distribution in splits\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTesting set class distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling/Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only (to prevent data leakage)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"Scaled training data shape:\", X_train_scaled.shape)\n",
    "print(\"Scaled testing data shape:\", X_test_scaled.shape)\n",
    "print(\"\\nSample of scaled training data:\")\n",
    "print(X_train_scaled.head())\n",
    "print(\"\\nScaled data statistics:\")\n",
    "print(X_train_scaled.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of scaling on a few features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "features_to_plot = ['radius_mean', 'texture_mean', 'perimeter_mean', \n",
    "                    'area_mean', 'smoothness_mean', 'compactness_mean']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    # Before scaling\n",
    "    axes[row, col].hist(X_train[feature], bins=30, alpha=0.5, label='Original', color='blue')\n",
    "    # After scaling\n",
    "    axes[row, col].hist(X_train_scaled[feature], bins=30, alpha=0.5, label='Scaled', color='red')\n",
    "    axes[row, col].set_title(feature)\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Before and After Feature Scaling', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data for use in other notebooks\n",
    "import pickle\n",
    "\n",
    "# Create a dictionary with all preprocessed data\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_names': list(X.columns)\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"âœ“ Preprocessed data saved to 'preprocessed_data.pkl'\")\n",
    "print(\"\\nYou can load this data in other notebooks using:\")\n",
    "print(\"\"\"\\nimport pickle\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "X_train_scaled = data['X_train_scaled']\n",
    "y_train = data['y_train']\n",
    "# etc...\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n Original Dataset:\")\n",
    "print(f\"   - Total samples: {len(data)}\")\n",
    "print(f\"   - Number of features: {X.shape[1]}\")\n",
    "print(f\"   - Missing values: {data.isnull().sum().sum()}\")\n",
    "print(f\"   - Duplicate rows: {data.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\n Target Variable:\")\n",
    "print(f\"   - Benign (B/0): {(y==0).sum()} ({(y==0).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"   - Malignant (M/1): {(y==1).sum()} ({(y==1).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"   - Class imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "print(f\"\\n Train-Test Split:\")\n",
    "print(f\"   - Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Stratified: Yes\")\n",
    "\n",
    "print(f\"\\n Feature Engineering:\")\n",
    "print(f\"   - Scaling method: StandardScaler (z-score normalization)\")\n",
    "print(f\"   - Highly correlated pairs (|r|>0.9): {len(high_corr_pairs)}\")\n",
    "print(f\"   - Features removed: 1 (id)\")\n",
    "print(f\"   - Final feature count: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\n Output Files:\")\n",
    "print(f\"   - preprocessed_data.pkl\")\n",
    "\n",
    "print(f\"\\n Data is ready for modeling!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
